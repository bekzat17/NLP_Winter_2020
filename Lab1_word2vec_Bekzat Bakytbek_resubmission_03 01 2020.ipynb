{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kx5X-m5AK2c9"
   },
   "outputs": [],
   "source": [
    "#Lab_1 prepared by Bekzat Bakytbek - resubmission\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re, string\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "d = collections.defaultdict(int)\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>berlin cheers for anti-nazi film a german movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>business</td>\n",
       "      <td>virgin blue shares plummet 20% shares in austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>business</td>\n",
       "      <td>crude oil prices back above $50 cold weather a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>politics</td>\n",
       "      <td>hague  given up  his pm ambition former conser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>sport</td>\n",
       "      <td>moya emotional after davis cup win carlos moya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>business</td>\n",
       "      <td>s korean credit card firm rescued south korea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>politics</td>\n",
       "      <td>howard backs stem cell research michael howard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>sport</td>\n",
       "      <td>connors boost for british tennis former world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>business</td>\n",
       "      <td>japanese banking battle at an end japan s sumi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>tech</td>\n",
       "      <td>games maker fights for survival one of britain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                               text\n",
       "0            tech  tv future in the hands of viewers with home th...\n",
       "1        business  worldcom boss  left books alone  former worldc...\n",
       "2           sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3           sport  yeading face newcastle in fa cup premiership s...\n",
       "4   entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "5        politics  howard hits back at mongrel jibe michael howar...\n",
       "6        politics  blair prepares to name poll date tony blair is...\n",
       "7           sport  henman hopes ended in dubai third seed tim hen...\n",
       "8           sport  wilkinson fit to face edinburgh england captai...\n",
       "9   entertainment  last star wars  not for children  the sixth an...\n",
       "10  entertainment  berlin cheers for anti-nazi film a german movi...\n",
       "11       business  virgin blue shares plummet 20% shares in austr...\n",
       "12       business  crude oil prices back above $50 cold weather a...\n",
       "13       politics  hague  given up  his pm ambition former conser...\n",
       "14          sport  moya emotional after davis cup win carlos moya...\n",
       "15       business  s korean credit card firm rescued south korea ...\n",
       "16       politics  howard backs stem cell research michael howard...\n",
       "17          sport  connors boost for british tennis former world ...\n",
       "18       business  japanese banking battle at an end japan s sumi...\n",
       "19           tech  games maker fights for survival one of britain..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the csv file using pandas\n",
    "bbc_dataframe = pd.read_csv('bbc_text.csv')\n",
    "#function returns first 20 elements of dataframe called bbc\n",
    "bbc_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that filters raw data (bbc data) into clean tokens removing punctuations, \n",
    "#numbers & converting sentences to lowercases\n",
    "def clean_corpus(sentence):\n",
    "    sentence = sentence.lower() #conveting to lowercase\n",
    "    sentence = re.sub(r'\\[.*?\\]', '', sentence) #removing punctuations\n",
    "    sentence = re.sub(r'[%s]' % re.escape(string.punctuation), '', sentence) #removing punctuations\n",
    "    sentence = re.sub(r'\\w*\\d\\w*', '', sentence) #removing punctuations\n",
    "    if len(sentence) > 2:\n",
    "        return ' '.join(w for w in sentence.split() if w not in STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tv future hands viewers home theatre systems p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>tigers wary farrell gamble leicester say rushe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ocean twelve raids box office ocean twelve cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>howard hits back mongrel jibe michael howard s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>blair prepares name poll date tony blair likel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>henman hopes ended dubai third seed tim henman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>wilkinson fit face edinburgh england captain j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>last star wars children sixth final star wars ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>berlin cheers antinazi film german movie antin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>virgin blue shares plummet shares australian b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>crude oil prices back cold weather across part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>hague given pm ambition former conservative le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>moya emotional davis cup win carlos moya descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>korean credit card firm rescued south korea la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>howard backs stem cell research michael howard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>connors boost british tennis former world numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>japanese banking battle end japan sumitomo mit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>games maker fights survival one britain larges...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   tv future hands viewers home theatre systems p...\n",
       "1   worldcom boss left books alone former worldcom...\n",
       "2   tigers wary farrell gamble leicester say rushe...\n",
       "3   yeading face newcastle fa cup premiership side...\n",
       "4   ocean twelve raids box office ocean twelve cri...\n",
       "5   howard hits back mongrel jibe michael howard s...\n",
       "6   blair prepares name poll date tony blair likel...\n",
       "7   henman hopes ended dubai third seed tim henman...\n",
       "8   wilkinson fit face edinburgh england captain j...\n",
       "9   last star wars children sixth final star wars ...\n",
       "10  berlin cheers antinazi film german movie antin...\n",
       "11  virgin blue shares plummet shares australian b...\n",
       "12  crude oil prices back cold weather across part...\n",
       "13  hague given pm ambition former conservative le...\n",
       "14  moya emotional davis cup win carlos moya descr...\n",
       "15  korean credit card firm rescued south korea la...\n",
       "16  howard backs stem cell research michael howard...\n",
       "17  connors boost british tennis former world numb...\n",
       "18  japanese banking battle end japan sumitomo mit...\n",
       "19  games maker fights survival one britain larges..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source https://thispointer.com/pandas-apply-apply-a-function-to-each-row-column-in-dataframe/\n",
    "filtered_corpus = pd.DataFrame(bbc_dataframe.text.apply(lambda x: clean_corpus(x)))\n",
    "#print filtered corpus\n",
    "filtered_corpus.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that applies lemmatizer using spacy\n",
    "#source: https://spacy.io/usage/models\n",
    "def lemmatizer(sentence):        \n",
    "    lemma = []\n",
    "    corpus_doc = nlp(sentence)\n",
    "    for w in corpus_doc:\n",
    "        lemma.append(w.lemma_)\n",
    "    return lemma\n",
    "\n",
    "#source https://thispointer.com/pandas-apply-apply-a-function-to-each-row-column-in-dataframe/\n",
    "filtered_corpus[\"text\"] =  filtered_corpus.apply(lambda x: lemmatizer(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[tv, future, hand, viewer, home, theatre, syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[worldcom, boss, leave, book, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[yeade, face, newcastle, fa, cup, premiership,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[ocean, twelve, raid, box, office, ocean, twel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[howard, hit, back, mongrel, jibe, michael, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[blair, prepare, name, poll, date, tony, blair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[henman, hope, end, dubai, third, seed, tim, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[wilkinson, fit, face, edinburgh, england, cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[last, star, war, child, sixth, final, star, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>[berlin, cheers, antinazi, film, german, movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>[virgin, blue, share, plummet, share, australi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>[crude, oil, price, back, cold, weather, acros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>[hague, give, pm, ambition, former, conservati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>[moya, emotional, davis, cup, win, carlos, moy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>[korean, credit, card, firm, rescue, south, ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>[howard, back, stem, cell, research, michael, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>[connor, boost, british, tennis, former, world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>[japanese, banking, battle, end, japan, sumito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>[game, maker, fight, survival, one, britain, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   [tv, future, hand, viewer, home, theatre, syst...\n",
       "1   [worldcom, boss, leave, book, alone, former, w...\n",
       "2   [tigers, wary, farrell, gamble, leicester, say...\n",
       "3   [yeade, face, newcastle, fa, cup, premiership,...\n",
       "4   [ocean, twelve, raid, box, office, ocean, twel...\n",
       "5   [howard, hit, back, mongrel, jibe, michael, ho...\n",
       "6   [blair, prepare, name, poll, date, tony, blair...\n",
       "7   [henman, hope, end, dubai, third, seed, tim, h...\n",
       "8   [wilkinson, fit, face, edinburgh, england, cap...\n",
       "9   [last, star, war, child, sixth, final, star, w...\n",
       "10  [berlin, cheers, antinazi, film, german, movie...\n",
       "11  [virgin, blue, share, plummet, share, australi...\n",
       "12  [crude, oil, price, back, cold, weather, acros...\n",
       "13  [hague, give, pm, ambition, former, conservati...\n",
       "14  [moya, emotional, davis, cup, win, carlos, moy...\n",
       "15  [korean, credit, card, firm, rescue, south, ko...\n",
       "16  [howard, back, stem, cell, research, michael, ...\n",
       "17  [connor, boost, british, tennis, former, world...\n",
       "18  [japanese, banking, battle, end, japan, sumito...\n",
       "19  [game, maker, fight, survival, one, britain, l..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_corpus.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25893"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source https://www.accelebrate.com/blog/using-defaultdict-python\n",
    "for w in filtered_corpus['text']:\n",
    "    for i in w:\n",
    "        d[i] += 1\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7GDVdmtKGG7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say', 'mr', 'year', 'would', 'make']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(d, key=d.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1568891818855,
     "user": {
      "displayName": "Александр Пак",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCoApcWJgCi9HMJv_dkQDj6kUy23W-cM3CGm_lX=s64",
      "userId": "15550640353612535544"
     },
     "user_tz": -360
    },
    "id": "q78TRvzUKQRd",
    "outputId": "6433da10-9703-4dca-9bba-e6c22d644687"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bekzat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north 0.8746350407600403\n",
      "south 0.8526561260223389\n",
      "east 0.8177163600921631\n",
      "trade 0.7740395069122314\n",
      "german 0.7724226117134094\n",
      "australia 0.7637919187545776\n",
      "annual 0.7452062964439392\n",
      "africa 0.7439569234848022\n",
      "germany 0.7369859218597412\n",
      "exchange 0.7268362045288086\n"
     ]
    }
   ],
   "source": [
    "#source https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "model = Word2Vec(size=1000, min_count=100, window=2, workers=3, iter = 10) #setting the paramethers\n",
    "model.build_vocab(filtered_corpus['text']) #build our vocabulary\n",
    "model.train(filtered_corpus['text'], total_examples=model.corpus_count, epochs=model.iter) #training our model\n",
    "model.init_sims(replace = True)\n",
    "model.save('word2vec_model')\n",
    "model = Word2Vec.load('word2vec_model')\n",
    "similarities = model.wv.most_similar('america')\n",
    "\n",
    "for word , score in similarities:\n",
    "    print(word , score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
